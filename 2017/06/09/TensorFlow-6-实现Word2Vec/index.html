<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">























  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext">
  






<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=6.7.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.7.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.7.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '6.7.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="本文首先对 Word2Vec 进行简单介绍，然后使用 python 构造训练样本，最后实现 Word2Vec，并产生可视化结果。 本文中涉及的所有代码均在 github.com/ywtail。查看运行过程可点击 这个链接 。 Word2Vec 简介Word2Vec 也称 Word Embeddings，中文比较普遍的叫法是“词向量”或“词嵌入”。Word2Vec 是一个可以将中文词转为向量形式表达">
<meta name="keywords" content="python,MachineLearning,TensorFlow">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorFlow (6): 实现Word2Vec">
<meta property="og:url" content="http://ywtail.github.io/2017/06/09/TensorFlow-6-实现Word2Vec/index.html">
<meta property="og:site_name" content="ywtail&#39;s blog">
<meta property="og:description" content="本文首先对 Word2Vec 进行简单介绍，然后使用 python 构造训练样本，最后实现 Word2Vec，并产生可视化结果。 本文中涉及的所有代码均在 github.com/ywtail。查看运行过程可点击 这个链接 。 Word2Vec 简介Word2Vec 也称 Word Embeddings，中文比较普遍的叫法是“词向量”或“词嵌入”。Word2Vec 是一个可以将中文词转为向量形式表达">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://7q5c08.com1.z0.glb.clouddn.com/tsne.png">
<meta property="og:updated_time" content="2019-04-22T07:36:19.250Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TensorFlow (6): 实现Word2Vec">
<meta name="twitter:description" content="本文首先对 Word2Vec 进行简单介绍，然后使用 python 构造训练样本，最后实现 Word2Vec，并产生可视化结果。 本文中涉及的所有代码均在 github.com/ywtail。查看运行过程可点击 这个链接 。 Word2Vec 简介Word2Vec 也称 Word Embeddings，中文比较普遍的叫法是“词向量”或“词嵌入”。Word2Vec 是一个可以将中文词转为向量形式表达">
<meta name="twitter:image" content="http://7q5c08.com1.z0.glb.clouddn.com/tsne.png">






  <link rel="canonical" href="http://ywtail.github.io/2017/06/09/TensorFlow-6-实现Word2Vec/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>TensorFlow (6): 实现Word2Vec | ywtail's blog</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">ywtail's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://ywtail.github.io/2017/06/09/TensorFlow-6-实现Word2Vec/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ywtail">
      <meta itemprop="description" content="Be a man.">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ywtail's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">TensorFlow (6): 实现Word2Vec

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2017-06-09 09:34:25" itemprop="dateCreated datePublished" datetime="2017-06-09T09:34:25+08:00">2017-06-09</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-04-22 15:36:19" itemprop="dateModified" datetime="2019-04-22T15:36:19+08:00">2019-04-22</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/TensorFlow/" itemprop="url" rel="index"><span itemprop="name">TensorFlow</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <a href="/2017/06/09/TensorFlow-6-实现Word2Vec/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/2017/06/09/TensorFlow-6-实现Word2Vec/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/2017/06/09/TensorFlow-6-实现Word2Vec/" class="leancloud_visitors" data-flag-title="TensorFlow (6): 实现Word2Vec">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">阅读次数：</span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本文首先对 Word2Vec 进行简单介绍，然后使用 python 构造训练样本，最后实现 Word2Vec，并产生可视化结果。</p>
<p>本文中涉及的所有代码均在 <a href="https://github.com/ywtail/TensorFlow/tree/master/6_%E5%AE%9E%E7%8E%B0Word2Vec" target="_blank" rel="noopener">github.com/ywtail</a>。查看运行过程可点击 <a href="https://ywtail.github.io/TensorFlow/6_word2vec.html">这个链接</a> 。</p>
<h3 id="Word2Vec-简介"><a href="#Word2Vec-简介" class="headerlink" title="Word2Vec 简介"></a>Word2Vec 简介</h3><p>Word2Vec 也称 Word Embeddings，中文比较普遍的叫法是“词向量”或“词嵌入”。Word2Vec 是一个可以将中文词转为向量形式表达（Vector Representations）的模型。</p>
<h4 id="为什么要把字词转为向量"><a href="#为什么要把字词转为向量" class="headerlink" title="为什么要把字词转为向量"></a>为什么要把字词转为向量</h4><p>图像、音频等数据天然可以编码并存储为稠密向量的形式，比如图片是像素点的稠密矩阵，音频可以转为声音信号的频谱数据。自然语言在 Word2Vec 出现之前，通常将字词转成离散的单独的符号，例如 “cat” 一词或可表示为 <code>Id537</code> ，而 “dog” 一词或可表示为 <code>Id143</code>。</p>
<p>这些符号编码毫无规律，无法提供不同词汇之间可能存在的关联信息。换句话说，在处理关于 “dogs” 一词的信息时，模型将无法利用已知的关于 “cats” 的信息（例如，它们都是动物，有四条腿，可作为宠物等等）。可见，将词汇表达为上述的独立离散符号将进一步导致数据稀疏，使我们在训练统计模型时不得不寻求更多的数据。而词汇的向量表示将克服上述的难题。</p>
<p><a href="https://en.wikipedia.org/wiki/Vector_space_model" target="_blank" rel="noopener">向量空间模型</a> (Vector Space Models，VSMs)将词汇表达（嵌套）于一个连续的向量空间中，语义近似的词汇被映射为相邻的数据点。向量空间模型在自然语言处理领域中有着漫长且丰富的历史，不过几乎所有利用这一模型的方法都依赖于 <a href="https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis" target="_blank" rel="noopener">分布式假设</a>，其核心思想为<strong>出现于上下文情景中的词汇都有相类似的语义</strong>。采用这一假设的研究方法大致分为以下两类：<em>基于计数的方法</em> (e.g. <a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis" target="_blank" rel="noopener">潜在语义分析</a>)， 和 <em>预测方法</em> (e.g. <a href="http://www.scholarpedia.org/article/Neural_net_language_models" target="_blank" rel="noopener">神经概率化语言模型</a>).</p>
<p>其中它们的区别在如下论文中又详细阐述 <a href="http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf" target="_blank" rel="noopener">Baroni et al.</a>，不过简而言之：基于计数的方法计算某词汇与其邻近词汇在一个大型语料库中共同出现的频率及其他统计量，然后将这些统计量映射到一个小型且稠密的向量中。预测方法则试图直接从某词汇的邻近词汇对其进行预测，在此过程中利用已经学习到的小型且稠密的<em>嵌套向量</em>。</p>
<p>Word2vec 是一种可以进行高效率词嵌套学习的预测模型。其两种变体分别为：连续词袋模型（Continuous Bag of Words，CBOW）及 Skip-Gram 模型。从算法角度看，这两种方法非常相似，其区别为 CBOW 根据源词上下文词汇（’the cat sits on the’）来预测目标词汇（例如，‘mat’），而 Skip-Gram 模型做法相反，它通过目标词汇来预测源词汇。CBOW 对小型数据比较合适，而 Skip-Gram 在大型语料中表现得更好。</p>
<p>预测模型通常使用最大似然的方法，在给定前面的语句 h 的情况下，最大化目标词汇 w 的概率。但它存在的一个比较严重的问题是计算量非常大，需要计算词汇表中所有单词出现的可能性。在 Word2Vec 的 CBOW 模型中，不需要计算完整的概率模型，只需要训练一个二元的分类模型，用来区分真实的目标词汇和编造的词汇（噪声）这两类。用这种少量噪声词汇来估计的方法，类似于蒙特卡洛模拟。</p>
<p>当模型预测真实的目标词汇为高概率，同时预测其他噪声词汇为低概率时，我们训练的学习目标就被最优化了。用编造的噪声词汇训练的方法被称为 负采样（ <code>Negative Sampling</code>），用这种方法计算 loss function 的效率非常高，我们只需要计算随机选择的 k 个词汇而非词汇表中的全部词汇，因此训练速度非常快。在实际中，我们使用 <code>Noise-Contrastive Estimation(NCE) Loss</code>，同时在 TensorFlow 中也有 <code>tf.nn.nce_loss()</code> 直接实现了这个 loss。在本节中我们将主要实现 Skip-Gram 模式的 Word2Vec。</p>
<p>更具体的信息参见：<a href="http://www.tensorfly.cn/tfdoc/tutorials/word2vec.html" target="_blank" rel="noopener">TensorFow 中国社区：Vector Representations of Words</a></p>
<h4 id="Skip-gram-模型"><a href="#Skip-gram-模型" class="headerlink" title="Skip-gram 模型"></a>Skip-gram 模型</h4><p>下面来看一下这个数据集：<code>the quick brown fox jumped over the lazy dog</code></p>
<p>我们首先对一些单词以及它们的上下文环境建立一个数据集。我们可以以任何合理的方式定义‘上下文’，而通常上这个方式是根据文字的句法语境的（使用语法原理的方式处理当前目标单词可以看一下这篇文献 <a href="https://levyomer.files.wordpress.com/2014/04/dependency-based-word-embeddings-acl-2014.pdf" target="_blank" rel="noopener">Levy et al.</a>，比如说把目标单词左边的内容当做一个‘上下文’，或者以目标单词右边的内容，等等。现在我们把目标单词的左右单词视作一个上下文， 使用大小为1的窗口，这样就得到这样一个由<code>(上下文, 目标单词)</code> 组成的数据集：<code>([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox), ...</code></p>
<p>前文提到Skip-Gram模型是把目标单词和上下文颠倒过来，所以在这个问题中，举个例子，就是用’quick’来预测 ‘the’ 和 ‘brown’ ，用 ‘brown’ 预测 ‘quick’ 和 ‘brown’ 。因此这个数据集就变成由<code>(输入, 输出)</code>组成的：<code>(quick, the), (quick, brown), (brown, quick), (brown, fox), ...</code></p>
<p>目标函数通常是对整个数据集建立的，但是本问题中要对每一个样本（或者是一个<code>batch_size</code> 很小的样本集，通常设置为<code>16 &lt;= batch_size &lt;= 512</code>）在同一时间执行特别的操作，称之为<a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" target="_blank" rel="noopener">随机梯度下降</a> (SGD)。</p>
<h3 id="构造训练样本"><a href="#构造训练样本" class="headerlink" title="构造训练样本"></a>构造训练样本</h3><p>实现 Word2Vec 首先需要构造训练样本。以 <code>the quick brown fox jumped over the lazy dog</code> 为例，我们需要构造一个语境与目标词汇的映射关系，假设我们的滑动窗口尺寸为 1，则语境包括一个单词左边和右边的词汇，可以制造的映射关系包括 <code>[the, brown] -&gt; quick, [quick, fox] -&gt; brown, [brown, jumped] -&gt; fox</code> 等。</p>
<p>因为 Skip-Gram 模型是从目标词汇预测语境，所以训练样本不再是 <code>[the, brown] -&gt; quick</code>，而是 <code>quick -&gt; the</code> 和 <code>quick -&gt; brown</code>。我们的数据集就变为了 <code>(quick, the)、(quick, brown)、(brown, quick)、(brown, fox)</code> 等。</p>
<p>我们训练时，希望模型能从目标词汇 quick 预测出语境 the，同时也需要制造随机的词汇作为负样本（噪声），我们希望预测的概率分布在正样本 the 上尽可能大，而在随机产生的负样本上尽可能小。这里的做法就是通过优化算法（例如 SGC）来更新模型中的 Word Embedding 的参数，让概率分布的损失函数（NCE Loss）尽可能小。这样每个单词的 Embedded Vector 就会随着就训练过程不断调整，直到出于一个最合适语料的空间位置。这样我们的损失函数最小，最符合语料，同时预测出正确单词的概率也最高。</p>
<h4 id="下载数据集"><a href="#下载数据集" class="headerlink" title="下载数据集"></a>下载数据集</h4><p>数据集获取有两种方法</p>
<ul>
<li>在浏览器地址栏输入 <a href="http://mattmahoney.net/dc/text8.zip" target="_blank" rel="noopener">http://mattmahoney.net/dc/text8.zip</a> 下载数据的压缩文件。</li>
<li><p>使用 <code>urllib.urlretrieve</code> 下载数据的压缩文件，并核对尺寸，如果已经下载了文件则跳过。代码如下，下载成功提示 <code>(&#39;Found and verified&#39;, &#39;text8.zip&#39;)</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybe_download</span><span class="params">(filename, expected_bytes)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(filename):</span><br><span class="line">        filename, _ = urllib.urlretrieve(url + filename, filename)</span><br><span class="line">    statinfo = os.stat(filename)</span><br><span class="line">    <span class="keyword">if</span> statinfo.st_size == expected_bytes:</span><br><span class="line">        print(<span class="string">'Found and verified'</span>, filename)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(statinfo.st_size)</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">'Failed to verify '</span> + filename + <span class="string">'. Can you get to it with a browser?'</span>)</span><br><span class="line">    <span class="keyword">return</span> filename</span><br><span class="line"></span><br><span class="line">filename = maybe_download(<span class="string">'text8.zip'</span>, <span class="number">31344016</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="解压并转为列表"><a href="#解压并转为列表" class="headerlink" title="解压并转为列表"></a>解压并转为列表</h4><p>接下来解压（使用 <code>zipfile.ZipFile</code> 函数）下载的压缩文件，并使用 <code>tf.compat.as_str</code> 将数据转成单词的列表。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> zipfile.ZipFile(filename) <span class="keyword">as</span> f:</span><br><span class="line">        data = tf.compat.as_str(f.read(f.namelist()[<span class="number">0</span>])).split()</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">words = read_data(filename)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'Data size'</span>, len(words)</span><br></pre></td></tr></table></figure>
<p>通过输出知道数据最后被转为了一个包含 17005207 个单词的列表。</p>
<h4 id="创建词汇表"><a href="#创建词汇表" class="headerlink" title="创建词汇表"></a>创建词汇表</h4><p>使用 <code>collections.Counter</code> 统计单词的频数，然后使用 <code>most_common</code> 方法获取词频数最高的 50000 个单词加入词汇表。因为 python 中字典查询复杂度为 O(1)，性能非常好，所以将词汇表 dictionary 设置为字典 ，将词频最高的50000 个词汇放入 dictionary 中，以便快速查询。接下来将全部单词转为编号（以频数排序的编号），top 50000 之外的词，认定其为 Unkown（未知），将其编号为0。<br>最后返回：</p>
<ul>
<li>data：转换后的编码</li>
<li>count：每个单词的频数统计</li>
<li>dictionary：词汇表（词：编码）</li>
<li>reverse_dictionary：词汇表的反转形式（编码：词）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">vocabulary_size = <span class="number">50000</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dataset</span><span class="params">(words)</span>:</span></span><br><span class="line">    count = [[<span class="string">'UNK'</span>, <span class="number">-1</span>]]  <span class="comment"># 前面是词汇，后面是出现的次数</span></span><br><span class="line">    count.extend(collections.Counter(words).most_common(vocabulary_size - <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    dictionary = dict()</span><br><span class="line">    <span class="keyword">for</span> word, _ <span class="keyword">in</span> count:</span><br><span class="line">        dictionary[word] = len(dictionary)</span><br><span class="line"></span><br><span class="line">    data = list()  <span class="comment"># 转换后的编码：如果出现在 dictionary 中，数量作为编号，不出现 0 作为编号</span></span><br><span class="line">    unk_count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> dictionary:</span><br><span class="line">            index = dictionary[word]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            index = <span class="number">0</span></span><br><span class="line">            unk_count += <span class="number">1</span></span><br><span class="line">        data.append(index)</span><br><span class="line"></span><br><span class="line">    count[<span class="number">0</span>][<span class="number">1</span>] = unk_count</span><br><span class="line">    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))</span><br><span class="line">    <span class="keyword">return</span> data, count, dictionary, reverse_dictionary</span><br><span class="line"></span><br><span class="line">data, count, dictionary, reverse_dictionary = build_dataset(words)</span><br></pre></td></tr></table></figure>
<h4 id="生成-Word2Vec-训练样本"><a href="#生成-Word2Vec-训练样本" class="headerlink" title="生成 Word2Vec 训练样本"></a>生成 Word2Vec 训练样本</h4><p>根据前面提到的 Skip-Gram 模式（从目标单词反推语境），定义 <code>generate_batch</code> 函数来生成训练用的 batch 数据，将原始数据 <code>the quick brown fox jumped over the lazy dog</code> 转为形如 <code>(quick, the)、(quick, brown)、(brown, quick)、(brown, fox)</code> 的样本。</p>
<p>在 <code>generate_batch(batch_size, num_skips, skip_window)</code> 函数中，变量含义如下：</p>
<ul>
<li>batch_size： batch 的大小；</li>
<li>skip_window ：单词间最远可以联系到的距离，设为 1 代表只能跟紧相邻的一个单词生成样本，例如 quick 只能生成 (quick, the) 和 (quick, brown)；</li>
<li>num_skips：对每个目标单词提取的样本数，它不能大于 skip_window 的两倍，并且 batch_size 必须是它的整数倍（为了确保每个 batch 包含了一个词汇对应的所有样本）；</li>
<li>data_index：单词序号，初始化为0，是global 变量，因为会反复调用 generate_batch，所以要确保 data_index 可以在函数 generate_batch 中被修改；</li>
</ul>
<p>在函数  <code>generate_batch</code> 中，使用 assert 断言确保 skip_window 和 num_skips 满足前面提到的条件：<code>batch_size % num_skips == 0</code>，<code>num_skips &lt;= 2 * skip_window</code>。</p>
<p>断言 <code>batch_size % num_skips == 0</code> 是为了确保每个 batch 包含了一个词汇对应的所有样本。</p>
<p>断言 <code>num_skips &lt;= 2 * skip_window</code> 是因为：假设词汇表words中的词是 <code>a, b, c, d, e, f</code>，skip_window=2 （单词间最远可以联系到的距离），那么显然，num_skips 最大是 4。假设 c 是目标词汇 c -&gt; a, b、c -&gt; d, e、b -&gt; c, d、d -&gt; b, c，对目标单词 c 提取的样本最多为 4 个。如果目标单词是 a，则不足 4 个。但是在下方的代码中，目标词汇取的是 buffer[skip_window]，并不是从 buffer[0] 开始的。如果目标单词是 e，因为 <code>data_index = (data_index + 1) % len(data)</code>，所以此时 buffer 中是[c, d, e, f, a]，取的样本依然是 4 个。所以 <code>num_skips &lt;= 2 * skip_window</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data_index = <span class="number">0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_batch</span><span class="params">(batch_size, num_skips, skip_window)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> data_index</span><br><span class="line">    <span class="keyword">assert</span> batch_size % num_skips == <span class="number">0</span></span><br><span class="line">    <span class="keyword">assert</span> num_skips &lt;= <span class="number">2</span> * skip_window</span><br></pre></td></tr></table></figure>
<p>定义输入特征和输出：</p>
<ul>
<li>batch：输入特征 features，用 np.ndarray 将 batch 初始化为数组；</li>
<li>labels：输出 labels，用 np.ndarray 将 labels 初始化为数组；</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch = np.ndarray(shape=(batch_size), dtype=np.int32)</span><br><span class="line">labels = np.ndarray(shape=(batch_size, <span class="number">1</span>), dtype=np.int32)</span><br></pre></td></tr></table></figure>
<p>定义双向队列 buffer，用于存储长度为 span 的单词编号：</p>
<ul>
<li>span：对某个单词创建相关样本时会使用到的单词数量，包括目标单词本身和它前后的单词，因此  <code>span=2*skip_window+1</code>；</li>
<li>buffer：容量为 span 的 deque（双向队列），在用 append 对 deque 添加变量时，只会保留最后插入的 span 个变量，其中存的是词的编号；</li>
</ul>
<p>在函数  <code>generate_batch</code> 中，创建最大容量为 span 的 deque，从 data_index 开始，把 span 个单词顺序读入 buffer 作为初始值，buffer 中存的是词的编号。因为 buffer 是容量为 span 的 deque，所以此时 buffer 已经充满，后续数据将替换掉前面的数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">span = <span class="number">2</span> * skip_window + <span class="number">1</span></span><br><span class="line">buffer = collections.deque(maxlen=span)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(span):</span><br><span class="line">    buffer.append(data[data_index])</span><br><span class="line">    data_index = (data_index + <span class="number">1</span>) % len(data)</span><br></pre></td></tr></table></figure>
<p>然后我们进入第一层循环（次数为 <code>batch_size // num_skips</code>），每次循环内对一个目标单词生成样本。现在 buffer 中是目标单词和所有相关单词，我们定义 <code>target=skip_window</code>，即 buffer 中第 skip_window 个单词为目标单词。然后定义生成样本时需要避免的单词列表 targets_to_avoid，这个列表开始包括第 skip_window 个单词（即目标单词），因为我们要预测的是语境单词，不包括目标单词本身。</p>
<p>接下来进入第二层循环（次数为 num_skips），每次循环对一个语境单词生成样本， 先产生一个随机数，直到随机数不在 targets_to_avoid 中，就可以将之作为语境单词。feature 是目标词汇 buffer[skip_window]，label 是 buffer[target]。同时，因为这个语境单词被使用了，所以再把它添加到 targets_to_avoid 中过滤。在对一个目标单词生生成完所有样本后（num_skips 个样本），我们再读入下一个单词（同时会抛掉 buffer 中第一个单词），即把滑窗向后移动一位，这样我们的目标单词也向后移动了一个，语境单词也整体后移了，便可以开始生成下一个目标单词的训练样本。</p>
<p>两层循环完成后，我们已经获得了 batch_size 个训练样本，将 batch 和 labels 作为函数结果返回。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size // num_skips):</span><br><span class="line">    target = skip_window</span><br><span class="line">    targets_to_avoid = [skip_window]</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(num_skips):</span><br><span class="line">        <span class="keyword">while</span> target <span class="keyword">in</span> targets_to_avoid:</span><br><span class="line">            target = random.randint(<span class="number">0</span>, span - <span class="number">1</span>)</span><br><span class="line">        targets_to_avoid.append(target)</span><br><span class="line">        batch[i * num_skips + j] = buffer[skip_window]</span><br><span class="line">        labels[i * num_skips + j, <span class="number">0</span>] = buffer[target]</span><br><span class="line">    buffer.append(data[data_index])</span><br><span class="line">    data_index = (data_index + <span class="number">1</span>) % len(data)</span><br><span class="line"><span class="keyword">return</span> batch, labels</span><br></pre></td></tr></table></figure>
<p>到目前为止，我们对训练数据的生成完成，接下来实现 Word2Vec。</p>
<h3 id="实现-Word2Vec"><a href="#实现-Word2Vec" class="headerlink" title="实现 Word2Vec"></a>实现 Word2Vec</h3><h4 id="定义训练时需要的参数"><a href="#定义训练时需要的参数" class="headerlink" title="定义训练时需要的参数"></a>定义训练时需要的参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">embedding_size = <span class="number">128</span>  <span class="comment"># 将单词转为稠密向量的维度，一般是500~1000这个范围内的值，这里设为128</span></span><br><span class="line">skip_window = <span class="number">1</span>  <span class="comment"># 单词间最远可以联系到的距离</span></span><br><span class="line">num_skips = <span class="number">2</span>  <span class="comment"># 对每个目标单词提取的样本数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成验证数据，随机抽取一些频数最高的单词，看向量空间上跟它们距离最近的单词是否相关性比较高</span></span><br><span class="line">valid_size = <span class="number">16</span>  <span class="comment"># 抽取的验证单词数</span></span><br><span class="line">valid_window = <span class="number">100</span>  <span class="comment"># 验证单词只从频数最高的 100 个单词中抽取</span></span><br><span class="line">valid_examples = np.random.choice(valid_window, valid_size, replace=<span class="keyword">False</span>)  <span class="comment"># 随机抽取</span></span><br><span class="line">num_sampled = <span class="number">64</span>  <span class="comment"># 训练时用来做负样本的噪声单词的数量</span></span><br></pre></td></tr></table></figure>
<h4 id="定义-Skip-Gram-Word2Vec-模型网络结构"><a href="#定义-Skip-Gram-Word2Vec-模型网络结构" class="headerlink" title="定义 Skip-Gram Word2Vec 模型网络结构"></a>定义 Skip-Gram Word2Vec 模型网络结构</h4><p>Skip-Gram 模型有两个输入。一个是一组用整型表示的上下文单词，另一个是目标单词。给这些输入建立占位符节点，之后就可以填入数据了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建立输入占位符</span></span><br><span class="line">train_inputs = tf.placeholder(tf.int32, shape=[batch_size])</span><br><span class="line">train_labels = tf.placeholder(tf.int32, shape=[batch_size, <span class="number">1</span>])</span><br><span class="line">valid_dataset = tf.constant(valid_examples, dtype=tf.int32)  <span class="comment"># 将前面随机产生的 valid_examples 转为 TensorFlow 中的 constant</span></span><br></pre></td></tr></table></figure>
<p>这里谈得都是嵌套，那么需要定义一个嵌套参数矩阵。我们用唯一的随机值来初始化这个大矩阵。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机生成所有单词的词向量 embeddings，单词表大小 5000，向量维度 128</span></span><br><span class="line">embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], <span class="number">-1.0</span>, <span class="number">1.0</span>))</span><br></pre></td></tr></table></figure>
<p>对噪声-比对的损失计算就使用一个逻辑回归模型。对此，我们需要对语料库中的每个单词定义一个权重值和偏差值。(也可称之为<code>输出权重</code> 与之对应的 <code>输入嵌套值</code>)。定义如下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nce_weights = tf.Variable(</span><br><span class="line">            tf.truncated_normal([vocabulary_size, embedding_size], stddev=<span class="number">1.0</span> / math.sqrt(embedding_size)))</span><br><span class="line">nce_bias = tf.Variable(tf.zeros([vocabulary_size]))</span><br></pre></td></tr></table></figure>
<p>然后我们需要对批数据中的单词建立嵌套向量，TensorFlow 提供了方便的工具函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查找 train_inputs 对应的向量 embed</span></span><br><span class="line">embed = tf.nn.embedding_lookup(embeddings, train_inputs)</span><br></pre></td></tr></table></figure>
<p>现在我们有了每个单词的嵌套向量，接下来就是使用噪声-比对的训练方式来预测目标单词。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss = tf.reduce_mean(</span><br><span class="line">        tf.nn.nce_loss(weights=nce_weights, biases=nce_bias, labels=train_labels, inputs=embed, num_sampled=num_sampled,</span><br><span class="line">                       num_classes=vocabulary_size))</span><br></pre></td></tr></table></figure>
<p>我们对损失函数建立了图形节点，然后我们需要计算相应梯度和更新参数的节点，比如说在这里我们会使用随机梯度下降法，TensorFlow 也已经封装好了该过程。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">1.0</span>).minimize(loss)</span><br></pre></td></tr></table></figure>
<h4 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h4><p>定义最大的迭代次数为 10 万次，然后创建并设置默认的 session，并执行参数和初始化。在每一步迭代中，先使用 <code>generate_batch</code> 生成一个 batch 的 inputs 和 labels 数据，并用他们创建 feed_dict。然后使用 <code>session.run()</code> 执行一次优化器运算（即一次参数更新）和损失计算，并将这一步训练的 loss 积累到 <code>average_loss</code>。</p>
<p>为了观察运行过程，之后每2000 次循环，计算一个平均 loss 并显示出来。每 10000 次循环，计算一次验证单词与全部单词的相似度，并将每个验证单词最相近的 8 个单词显示出来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> session:</span><br><span class="line">    init.run()</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Initialized'</span></span><br><span class="line"></span><br><span class="line">    average_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(num_steps):</span><br><span class="line">        batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)</span><br><span class="line">        feed_dict = &#123;train_inputs: batch_inputs, train_labels: batch_labels&#125;</span><br><span class="line"></span><br><span class="line">        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)</span><br><span class="line">        average_loss += loss_val</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> step &gt; <span class="number">0</span>:</span><br><span class="line">                average_loss /= <span class="number">2000</span></span><br><span class="line">            <span class="keyword">print</span> <span class="string">'Average loss at step &#123;&#125; : &#123;&#125;'</span>.format(step, average_loss)</span><br><span class="line">            average_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">            sim = similarity.eval()</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(valid_size):</span><br><span class="line">                valid_word = reverse_dictionary[valid_examples[i]]</span><br><span class="line">                top_k = <span class="number">8</span></span><br><span class="line">                nearest = (-sim[i, :]).argsort()[<span class="number">1</span>:top_k + <span class="number">1</span>]</span><br><span class="line">                log_str = <span class="string">'Nearest to &#123;&#125; :'</span>.format(valid_word)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> range(top_k):</span><br><span class="line">                    close_word = reverse_dictionary[nearest[k]]</span><br><span class="line">                    log_str = <span class="string">'&#123;&#125; &#123;&#125;,'</span>.format(log_str, close_word)</span><br><span class="line">                <span class="keyword">print</span> log_str</span><br><span class="line">        final_embeddings = normalized_embeddings.eval()s</span><br></pre></td></tr></table></figure>
<p>运行的一部分结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Initialized</span><br><span class="line">Average loss at step 0 : 302.263977051</span><br><span class="line">Nearest to that : slime, armament, mtsho, reliever, apocrypha, vocalic, allosteric, usda,</span><br><span class="line">Nearest to in : oint, alvarado, centres, beavers, miura, processes, laud, lyricist,</span><br><span class="line">Nearest to history : howling, amalgamated, nupedia, wiener, tomahawk, quakers, profil, lactate,</span><br><span class="line">Nearest to used : foals, ueshiba, yazoo, frustrated, esters, deploy, vanity, affine,</span><br><span class="line">Nearest to than : travelled, inclination, occupational, kets, smoothing, pathways, solutions, zolt,</span><br><span class="line">Nearest to during : empiricists, png, worst, atman, ortelius, swayed, heaps, racks,</span><br><span class="line">Nearest to it : blockading, lyell, adjectives, karelian, fredericksburg, scatter, behe, ewes,</span><br><span class="line">Nearest to will : topple, lv, abram, challenged, osip, hst, corrupting, slammed,</span><br><span class="line">Nearest to which : prometheus, tropic, erosive, dai, haliotis, visualize, paradoxically, minors,</span><br><span class="line">Nearest to new : trajan, evacuate, melanogaster, eagerness, pam, flee, ferrol, strengthen,</span><br><span class="line">Nearest to one : heckel, pleadings, jam, washing, earnings, distinguishes, congratulate, lettering,</span><br><span class="line">Nearest to nine : cva, pictish, unruly, mysql, zinn, evangelists, vacancies, schedel,</span><br><span class="line">Nearest to more : dysrhythmias, nucleus, persistently, prophesies, samarkand, mojave, recreate, attempting,</span><br><span class="line">Nearest to its : pq, dreamed, robin, homesick, offensive, apostol, gur, companionship,</span><br><span class="line">Nearest to with : agm, cru, eos, nasal, litchfield, ccny, macross, exhorted,</span><br><span class="line">Nearest to after : bremen, ddrmax, lutherans, ward, deum, bracelets, crevasses, pv,</span><br><span class="line">Average loss at step 2000 : 114.135557295</span><br><span class="line">Average loss at step 4000 : 52.9445186524</span><br><span class="line">Average loss at step 6000 : 33.5687308327</span><br><span class="line">Average loss at step 8000 : 23.5443917145</span><br><span class="line">Average loss at step 10000 : 17.309888566</span><br><span class="line">Nearest to that : analogue, shoes, slime, motivating, austin, and, have, to,</span><br><span class="line">Nearest to in : and, of, with, by, from, nine, UNK, for,</span><br></pre></td></tr></table></figure>
<h4 id="结果可视化"><a href="#结果可视化" class="headerlink" title="结果可视化"></a>结果可视化</h4><p>下面定义一个用来可视化 Word2Vec 效果的函数。这里 <code>low_dim_embs</code> 是降维到 2 维 的单词的空间向量，我们将在图表中展示每个单词的位置。我么使用 <code>plt.scatter</code> 显示散点图（单词的位置），并用 <code>plt.annotate</code> 展示单词本身，同时，使用 <code>plt.savefig</code> 保存图片到本地文件。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_with_labels</span><span class="params">(low_dim_embs, labels, filename=<span class="string">'tsne.png'</span>)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> low_dim_embs.shape[<span class="number">0</span>] &gt;= len(labels), <span class="string">'More labels then embeddings'</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">18</span>, <span class="number">18</span>))</span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(labels):</span><br><span class="line">        x, y = low_dim_embs[i, :]</span><br><span class="line">        plt.scatter(x, y)</span><br><span class="line">        plt.annotate(label, xy=(x, y), xytext=(<span class="number">5</span>, <span class="number">2</span>), textcoords=<span class="string">'offset points'</span>, ha=<span class="string">'right'</span>, va=<span class="string">'bottom'</span>)</span><br><span class="line">    plt.savefig(filename)</span><br></pre></td></tr></table></figure>
<p>我们使用 <code>sklearn.manifold.TSNE</code> 实现降维，这里直接将原始的 128 维的嵌入向量降到 2 维，再用前面的 <code>plot_with_labels</code> 函数进行展示。这里只展示词频最高的 100 个单词的可视化结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">tsne = TSNE(perplexity=<span class="number">30</span>, n_components=<span class="number">2</span>, init=<span class="string">'pca'</span>, n_iter=<span class="number">5000</span>)</span><br><span class="line">plot_only = <span class="number">100</span></span><br><span class="line">low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])</span><br><span class="line">labels = [reverse_dictionary[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(plot_only)]</span><br><span class="line">plot_with_labels(low_dim_embs, labels)</span><br></pre></td></tr></table></figure>
<p>从可视化结果可以看出，距离相近的单词在语义上具有很高的相似性。在训练 Word2Vec 模型时，为了获得比较好的结构，我们可以使用大规模的语料库，同时需要对参数进行调试，选取最合适的值。</p>
<h3 id="完整代码及运行结果"><a href="#完整代码及运行结果" class="headerlink" title="完整代码及运行结果"></a>完整代码及运行结果</h3><p>本文相关内容在 <a href="https://github.com/ywtail/TensorFlow/tree/master/6_%E5%AE%9E%E7%8E%B0Word2Vec" target="_blank" rel="noopener">github.com/ywtail</a>中，完整代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载数据集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybe_download</span><span class="params">(filename, expected_bytes)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(filename):</span><br><span class="line">        filename, _ = urllib.urlretrieve(url + filename, filename)</span><br><span class="line">    statinfo = os.stat(filename)</span><br><span class="line">    <span class="keyword">if</span> statinfo.st_size == expected_bytes:</span><br><span class="line">        print(<span class="string">'Found and verified'</span>, filename)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(statinfo.st_size)</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">'Failed to verify '</span> + filename + <span class="string">'. Can you get to it with a browser?'</span>)</span><br><span class="line">    <span class="keyword">return</span> filename</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">filename = maybe_download(<span class="string">'text8.zip'</span>, <span class="number">31344016</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将词存入 word 列表中</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> zipfile.ZipFile(filename) <span class="keyword">as</span> f:</span><br><span class="line">        data = tf.compat.as_str(f.read(f.namelist()[<span class="number">0</span>])).split()</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">words = read_data(filename)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'Data size'</span>, len(words)</span><br><span class="line"></span><br><span class="line">vocabulary_size = <span class="number">50000</span>  <span class="comment"># 将出现频率最高的 50000 个单词放入 count 列表中，然后放入 dicionary 中</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dataset</span><span class="params">(words)</span>:</span></span><br><span class="line">    count = [[<span class="string">'UNK'</span>, <span class="number">-1</span>]]  <span class="comment"># 前面是词汇，后面是出现的次数，这里的 -1 在下面会填上 UNK 出现的频数</span></span><br><span class="line">    <span class="comment"># 将出现频次最高的 50000 个词存入count</span></span><br><span class="line">    count.extend(collections.Counter(words).most_common(vocabulary_size - <span class="number">1</span>))  <span class="comment"># -1 因为 UNK 已经占了一个了</span></span><br><span class="line"></span><br><span class="line">    dictionary = dict()</span><br><span class="line">    <span class="keyword">for</span> word, _ <span class="keyword">in</span> count:</span><br><span class="line">        dictionary[word] = len(dictionary)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    等价于，就是按 count 中词出现的顺序，分别给他们编号：0 1 2 ...</span></span><br><span class="line"><span class="string">        for i in vocabulary_size:</span></span><br><span class="line"><span class="string">            dictionary[count[i][0]]=i</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 编码：如果不出现在 dictionary 中，就以 0 作为编号，否则以 dictionary 中的编号编号</span></span><br><span class="line">    <span class="comment"># 也就是将 words 中的所有词的编号存在 data 中，顺带查一下 UNK 有多少个，以便替换 count 中的 -1</span></span><br><span class="line">    data = list()</span><br><span class="line">    unk_count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> dictionary:</span><br><span class="line">            index = dictionary[word]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            index = <span class="number">0</span></span><br><span class="line">            unk_count += <span class="number">1</span></span><br><span class="line">        data.append(index)</span><br><span class="line"></span><br><span class="line">    count[<span class="number">0</span>][<span class="number">1</span>] = unk_count</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 编号：词</span></span><br><span class="line">    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))</span><br><span class="line">    <span class="keyword">return</span> data, count, dictionary, reverse_dictionary</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data, count, dictionary, reverse_dictionary = build_dataset(words)</span><br><span class="line"></span><br><span class="line"><span class="keyword">del</span> words  <span class="comment"># 删除原始单词表，节约内存</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成 Word2Vec 训练样本</span></span><br><span class="line">data_index = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_batch</span><span class="params">(batch_size, num_skips, skip_window)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> data_index  <span class="comment"># 设为global 因为会反复 generate</span></span><br><span class="line">    <span class="keyword">assert</span> batch_size % num_skips == <span class="number">0</span></span><br><span class="line">    <span class="keyword">assert</span> num_skips &lt;= <span class="number">2</span> * skip_window</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将 batch 和 labels 初始化为数组</span></span><br><span class="line">    batch = np.ndarray(shape=(batch_size), dtype=np.int32)</span><br><span class="line">    labels = np.ndarray(shape=(batch_size, <span class="number">1</span>), dtype=np.int32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对某个单词创建相关样本时会使用到的单词数量，包括目标单词本身和它前后的单词</span></span><br><span class="line">    span = <span class="number">2</span> * skip_window + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建最大容量为 span 的 deque（双向队列）</span></span><br><span class="line">    <span class="comment"># 在用 append 对 deque 添加变量时，只会保留最后插入的 span 个变量</span></span><br><span class="line">    buffer = collections.deque(maxlen=span)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从 data_index 开始，把 span 个单词顺序读入 buffer 作为初始值，buffer 中存的是词的编号</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(span):</span><br><span class="line">        buffer.append(data[data_index])</span><br><span class="line">        data_index = (data_index + <span class="number">1</span>) % len(data)</span><br><span class="line">    <span class="comment"># buffer 容量是 span，所以此时 buffer 已经填满，后续的数据将替换掉前面的数据</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每次循环内对一个目标单词生成样本，前方已经断言能整除，这里使用 // 是为了保证结果是 int</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size // num_skips):  <span class="comment"># //除法只保留结果整数部分（python3中），python2中直接 /</span></span><br><span class="line">        <span class="comment"># 现在 buffer 中是目标单词和所有相关单词</span></span><br><span class="line">        target = skip_window  <span class="comment"># buffer 中第 skip_window 个单词为目标单词（注意第一个目标单词是 buffer[skip_window]，并不是 buffer[0]）</span></span><br><span class="line">        targets_to_avoid = [skip_window]  <span class="comment"># 接下来生成相关（上下文语境）单词，应将目标单词拒绝</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每次循环对一个语境单词生成样本</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(num_skips):</span><br><span class="line">            <span class="comment"># 先产生一个随机数，直到随机数不在 targets_to_avoid 中，就可以将之作为语境单词</span></span><br><span class="line">            <span class="keyword">while</span> target <span class="keyword">in</span> targets_to_avoid:</span><br><span class="line">                target = random.randint(<span class="number">0</span>, span - <span class="number">1</span>)</span><br><span class="line">            targets_to_avoid.append(target)  <span class="comment"># 因为这个语境单词被使用了，所以要加入到 targets_to_avoid</span></span><br><span class="line"></span><br><span class="line">            batch[i * num_skips + j] = buffer[skip_window]  <span class="comment"># feature 是目标词汇</span></span><br><span class="line">            labels[i * num_skips + j, <span class="number">0</span>] = buffer[target]  <span class="comment"># label 是 buffer[target]</span></span><br><span class="line"></span><br><span class="line">        buffer.append(data[data_index])</span><br><span class="line">        data_index = (data_index + <span class="number">1</span>) % len(data)</span><br><span class="line">    <span class="keyword">return</span> batch, labels</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练需要的参数</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">embedding_size = <span class="number">128</span>  <span class="comment"># 将单词转为稠密向量的维度，一般是500~1000这个范围内的值，这里设为128</span></span><br><span class="line">skip_window = <span class="number">1</span>  <span class="comment"># 单词间最远可以联系到的距离</span></span><br><span class="line">num_skips = <span class="number">2</span>  <span class="comment"># 对每个目标单词提取的样本数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成验证数据，随机抽取一些频数最高的单词，看向量空间上跟它们距离最近的单词是否相关性比较高</span></span><br><span class="line">valid_size = <span class="number">16</span>  <span class="comment"># 抽取的验证单词数</span></span><br><span class="line">valid_window = <span class="number">100</span>  <span class="comment"># 验证单词只从频数最高的 100 个单词中抽取</span></span><br><span class="line">valid_examples = np.random.choice(valid_window, valid_size, replace=<span class="keyword">False</span>)  <span class="comment"># 随机抽取</span></span><br><span class="line">num_sampled = <span class="number">64</span>  <span class="comment"># 训练时用来做负样本的噪声单词的数量</span></span><br><span class="line"></span><br><span class="line">graph = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> graph.as_default():</span><br><span class="line">    <span class="comment"># 建立输入占位符</span></span><br><span class="line">    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])</span><br><span class="line">    train_labels = tf.placeholder(tf.int32, shape=[batch_size, <span class="number">1</span>])</span><br><span class="line">    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)  <span class="comment"># 将前面随机产生的 valid_examples 转为 TensorFlow 中的 constant</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):  <span class="comment"># 限定所有计算在 CPU 上执行</span></span><br><span class="line">        <span class="comment"># 随机生成所有单词的词向量 embeddings，单词表大小 5000，向量维度 128</span></span><br><span class="line">        embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], <span class="number">-1.0</span>, <span class="number">1.0</span>))</span><br><span class="line">        <span class="comment"># 查找 train_inputs 对应的向量 embed</span></span><br><span class="line">        embed = tf.nn.embedding_lookup(embeddings, train_inputs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用 NCE Loss 作为训练的优化目标</span></span><br><span class="line">        nce_weights = tf.Variable(</span><br><span class="line">            tf.truncated_normal([vocabulary_size, embedding_size], stddev=<span class="number">1.0</span> / math.sqrt(embedding_size)))</span><br><span class="line">        nce_bias = tf.Variable(tf.zeros([vocabulary_size]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用 tf.nn.nce_loss 计算学习出的词向量 embed 在训练数据上的 loss，并使用 tf.reduce_mean 进行汇总</span></span><br><span class="line">    loss = tf.reduce_mean(</span><br><span class="line">        tf.nn.nce_loss(weights=nce_weights, biases=nce_bias, labels=train_labels, inputs=embed, num_sampled=num_sampled,</span><br><span class="line">                       num_classes=vocabulary_size))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义优化器为 SGD，且学习速率为 1.0</span></span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer(<span class="number">1.0</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算嵌入向量 embeddings 的 L2 范数 norm</span></span><br><span class="line">    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), <span class="number">1</span>, keep_dims=<span class="keyword">True</span>))</span><br><span class="line">    <span class="comment"># 标准化</span></span><br><span class="line">    normalized_embeddings = embeddings / norm</span><br><span class="line">    <span class="comment"># 查询验证单词的嵌入向量，并计算验证单词的嵌入向量与词汇表中所有单词的相似性</span></span><br><span class="line">    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)</span><br><span class="line">    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化所有模型参数</span></span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">num_steps = <span class="number">100001</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> session:</span><br><span class="line">    init.run()</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Initialized'</span></span><br><span class="line"></span><br><span class="line">    average_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(num_steps):</span><br><span class="line">        batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)</span><br><span class="line">        feed_dict = &#123;train_inputs: batch_inputs, train_labels: batch_labels&#125;</span><br><span class="line"></span><br><span class="line">        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)</span><br><span class="line">        average_loss += loss_val</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> step &gt; <span class="number">0</span>:</span><br><span class="line">                average_loss /= <span class="number">2000</span></span><br><span class="line">            <span class="keyword">print</span> <span class="string">'Average loss at step &#123;&#125; : &#123;&#125;'</span>.format(step, average_loss)</span><br><span class="line">            average_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">            sim = similarity.eval()</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(valid_size):</span><br><span class="line">                valid_word = reverse_dictionary[valid_examples[i]]</span><br><span class="line">                top_k = <span class="number">8</span></span><br><span class="line">                nearest = (-sim[i, :]).argsort()[<span class="number">1</span>:top_k + <span class="number">1</span>]</span><br><span class="line">                log_str = <span class="string">'Nearest to &#123;&#125; :'</span>.format(valid_word)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> range(top_k):</span><br><span class="line">                    close_word = reverse_dictionary[nearest[k]]</span><br><span class="line">                    log_str = <span class="string">'&#123;&#125; &#123;&#125;,'</span>.format(log_str, close_word)</span><br><span class="line">                <span class="keyword">print</span> log_str</span><br><span class="line">        final_embeddings = normalized_embeddings.eval()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_with_labels</span><span class="params">(low_dim_embs, labels, filename=<span class="string">'tsne.png'</span>)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> low_dim_embs.shape[<span class="number">0</span>] &gt;= len(labels), <span class="string">'More labels then embeddings'</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">18</span>, <span class="number">18</span>))</span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(labels):</span><br><span class="line">        x, y = low_dim_embs[i, :]</span><br><span class="line">        plt.scatter(x, y)</span><br><span class="line">        plt.annotate(label, xy=(x, y), xytext=(<span class="number">5</span>, <span class="number">2</span>), textcoords=<span class="string">'offset points'</span>, ha=<span class="string">'right'</span>, va=<span class="string">'bottom'</span>)</span><br><span class="line">    plt.savefig(filename)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tsne = TSNE(perplexity=<span class="number">30</span>, n_components=<span class="number">2</span>, init=<span class="string">'pca'</span>, n_iter=<span class="number">5000</span>)</span><br><span class="line">plot_only = <span class="number">100</span></span><br><span class="line">low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])</span><br><span class="line">labels = [reverse_dictionary[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(plot_only)]</span><br><span class="line">plot_with_labels(low_dim_embs, labels)</span><br></pre></td></tr></table></figure>
<p>生成的可视化文件如下：<br><img src="http://7q5c08.com1.z0.glb.clouddn.com/tsne.png" alt="word2vec可视化"></p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul>
<li>图书：TensorFlow实战 / 黄文坚，唐源著</li>
<li>TensorFlow 中文社区：<a href="http://www.tensorfly.cn/tfdoc/tutorials/word2vec.html" target="_blank" rel="noopener">Vector Representations of Words</a></li>
</ul>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/python/" rel="tag"># python</a>
          
            <a href="/tags/MachineLearning/" rel="tag"># MachineLearning</a>
          
            <a href="/tags/TensorFlow/" rel="tag"># TensorFlow</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/06/08/sklearn决策树可视化/" rel="next" title="sklearn决策树可视化">
                <i class="fa fa-chevron-left"></i> sklearn决策树可视化
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/06/19/Kaggle记录/" rel="prev" title="Kaggle记录">
                Kaggle记录 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">ywtail</p>
              <p class="site-description motion-element" itemprop="description">Be a man.</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">69</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">17</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">24</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Word2Vec-简介"><span class="nav-number">1.</span> <span class="nav-text">Word2Vec 简介</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么要把字词转为向量"><span class="nav-number">1.1.</span> <span class="nav-text">为什么要把字词转为向量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Skip-gram-模型"><span class="nav-number">1.2.</span> <span class="nav-text">Skip-gram 模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#构造训练样本"><span class="nav-number">2.</span> <span class="nav-text">构造训练样本</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#下载数据集"><span class="nav-number">2.1.</span> <span class="nav-text">下载数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#解压并转为列表"><span class="nav-number">2.2.</span> <span class="nav-text">解压并转为列表</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#创建词汇表"><span class="nav-number">2.3.</span> <span class="nav-text">创建词汇表</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#生成-Word2Vec-训练样本"><span class="nav-number">2.4.</span> <span class="nav-text">生成 Word2Vec 训练样本</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#实现-Word2Vec"><span class="nav-number">3.</span> <span class="nav-text">实现 Word2Vec</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#定义训练时需要的参数"><span class="nav-number">3.1.</span> <span class="nav-text">定义训练时需要的参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#定义-Skip-Gram-Word2Vec-模型网络结构"><span class="nav-number">3.2.</span> <span class="nav-text">定义 Skip-Gram Word2Vec 模型网络结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#训练模型"><span class="nav-number">3.3.</span> <span class="nav-text">训练模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#结果可视化"><span class="nav-number">3.4.</span> <span class="nav-text">结果可视化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#完整代码及运行结果"><span class="nav-number">4.</span> <span class="nav-text">完整代码及运行结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#参考"><span class="nav-number">5.</span> <span class="nav-text">参考</span></a></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ywtail</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v6.7.0</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="post-meta-divider">|</span>
  

  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=6.7.0"></script>

  <script src="/js/src/motion.js?v=6.7.0"></script>



  
  


  <script src="/js/src/schemes/muse.js?v=6.7.0"></script>




  
  <script src="/js/src/scrollspy.js?v=6.7.0"></script>
<script src="/js/src/post-details.js?v=6.7.0"></script>



  


  <script src="/js/src/bootstrap.js?v=6.7.0"></script>



  
  

<script src="//cdn1.lncld.net/static/js/3.11.1/av-min.js"></script>



<script src="//unpkg.com/valine/dist/Valine.min.js"></script>

<script>
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'Hn01Iq5tm1St1y55hF5K2M1P-gzGzoHsz',
    appKey: 'VMAeg3KLjfddIO4nxJCqersc',
    placeholder: '在此处输入评论',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false
  });
</script>




  


  

  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  
  
  <script>
    
    function addCount(Counter) {
      var $visitors = $('.leancloud_visitors');
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();

      Counter('get', '/classes/Counter', { where: JSON.stringify({ url }) })
        .done(function({ results }) {
          if (results.length > 0) {
            var counter = results[0];
            
            Counter('put', '/classes/Counter/' + counter.objectId, JSON.stringify({ time: { '__op': 'Increment', 'amount': 1 } }))
            
              .done(function() {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.time + 1);
              })
            
              .fail(function ({ responseJSON }) {
                console.log(`Failed to save Visitor num, with error message: ${responseJSON.error}`);
              })
          } else {
            
              Counter('post', '/classes/Counter', JSON.stringify({ title: title, url: url, time: 1 }))
                .done(function() {
                  var $element = $(document.getElementById(url));
                  $element.find('.leancloud-visitors-count').text(1);
                })
                .fail(function() {
                  console.log('Failed to create');
                });
            
          }
        })
        .fail(function ({ responseJSON }) {
          console.log(`LeanCloud Counter Error: ${responseJSON.code} ${responseJSON.error}`);
        });
    }
    

    $(function() {
      $.get('https://app-router.leancloud.cn/2/route?appId=' + 'Hn01Iq5tm1St1y55hF5K2M1P-gzGzoHsz')
        .done(function({ api_server }) {
          var Counter = function(method, url, data) {
            return $.ajax({
              method: method,
              url: 'https://' + api_server + '/1.1' + url,
              headers: {
                'X-LC-Id': 'Hn01Iq5tm1St1y55hF5K2M1P-gzGzoHsz',
                'X-LC-Key': 'VMAeg3KLjfddIO4nxJCqersc',
                'Content-Type': 'application/json',
              },
              data: data
            });
          };
          
            addCount(Counter);
          
        });
    });
  </script>



  

  

  

  
  

  
  

  


  

  

  

  

  

  

  

  

</body>
</html>
